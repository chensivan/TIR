# What is in this repository?

This repo saves my reading notes and thoughts on a list of selected academic papers and work related articles. For each article, I list the title, list of authors, and other relevant information. At the end of each item, you can find a hyperlink that links to my notes.

<!-- I also add visual tags to each paper. Here are what they meant:

ü§ñ: collaboration -->

# Reading list

NSF CAREER: Inquisitive Programming Environments as Learning Environments for Novices and Experts. Austin Henley. 2021

Polyjuice: Generating Counterfactuals for Explaining, Evaluating, and Improving Models. Wu, T., ACL-IJCNLP 2021.

# Notes of papers and articles I read

<em>(2022-2-27)</em> Austin, et al. Program synthesis with large language models arXiv:2108.07732 (2021). [Notes](notes/austin_arxiv2021/austin_arxiv2021.md)

<em>(2022-2-27)</em> Neural Language Models are Effective Plagiarists. Biderman, Stella, and Edward Raff. arXiv:2201.07406 (2022).
[Notes](notes/biderman_arxiv2022/biderman_arxiv2022.md)

<em>(2022-2-27)</em> Jigsaw: Large Language Models meet Program Synthesis. GJain, Naman et al. ICSE 2022. [Notes](notes/jain_icse2022/jain_icse2022.md)

<em>(2022-2-26)</em> Genline and genform: Two tools for interacting with generative language models in a code editor. Jiang, Ellen, et al. UIST 2021. [Notes](notes/jiang_uist2021/jiang_uist2021.md)

<em>(2022-2-26)</em> Sporq: An Interactive Environment for Exploring Code using Query-by-Example. Naik, Aaditya, et al. UIST 2021. [Notes](notes/naik_uist2021/naik_uist2021.md)

<em>(2022-2-25 )</em> Are visual explanations useful? a case study in model-in-the-loop prediction. E. Chu, et al. arXiv preprint arXiv:2007.12248 (2020). [Notes](notes/chu_archive2020/chu_archive2020.md)

<em>(2022-2-23)</em> Proxy tasks and subjective measures can be misleading in evaluating explainable AI systems Bu√ßinca, Zana, et a. IUI, 2020. [Notes](notes/zana_iui2020/zana_iui2020.md)

<em>(2022-2-21)</em> Onboarding Materials as Cross-functional Boundary Objects for Developing AI Assistants. Cai, Carrie J., et al. CHI 2021. [Notes](notes/cai_chi2021/cai_chi2021.md)

<em>(2022-2-20)</em> Promptiverse: Scalable Generation of Scaffolding Prompts through Human-AI Knowledge Graph Annotation. Yoonjoo Lee, et al. CHI2022 [Notes](notes/lee_chi2022/lee_chi2022.md)

<em>(2022-2-19)</em> PL and HCI: Better together. Sarah Chasins, et al. Communications of The ACM, 2021. [Notes](notes/chasins_comm2021/chasins_comm2021.md)

<em>(2022-2-19)</em> Loopholes: a Window into Value Alignment and the Learning of Meaning. Sophie Bridgers, et al. NeurIPS Workshop 2021. [Notes](notes/bridgers_nips2021/bridgers_nips2021.md)

<em>(2022-2-18)</em> Bot in the Bunch: Facilitating Group Chat Discussion by Improving Efficiency and Participation with a Chatbot. Soomin Kim, et al. CHI 2020. [Notes](notes/kim_chi2020/kim_chi2020.md)

<em>(2022-2-18)</em> Program synthesis using natural language.Desai, Aditya, Sumit Gulwani, Vineet Hingorani, Nidhi Jain, Amey Karkare, Mark Marron, and Subhajit Roy. ICSE 2016. [Notes](notes/desai_icse2016/desai_icse2016.md)

<em>(2022-2-16)</em> Why distance matters: effects on cooperation, persuasion and deception. Bradner, Erin, and Gloria Mark. CSCW 2002. [Notes](notes/bradner_cscw2002/bradner_cscw2002.md)

<em>(2022-2-15)</em> A multi-institutional study of peer instruction in introductory computing. Porter, Leo, Dennis Bouvier, Quintin Cutts, Scott Grissom, Cynthia Lee, Robert McCartney, Daniel Zingaro, and Beth Simon. SIGCSE 2016. [Notes](notes/porter_cse2016/porter_cse2016.md)

<em>(2022-2-14)</em> Guidelines for human-AI interaction. Amershi, S., Weld, D., Vorvoreanu, M., Fourney, A., Nushi, B., Collisson, P., Suh, J., Iqbal, S., Bennett, P.N., Inkpen, K. and Teevan, J. CHI 2019. [Notes](notes/amershi_chi2019/amershi_chi2019.md)

<em>(2022-2-13)</em> Program synthesis with pragmatic communication. Pu Y, Ellis K, Kryven M, Tenenbaum J, Solar-Lezama A. NeurIPS 2020. [Notes](notes/pu_nips2020/pu_nips2020.md)

<!-- <em>(Feb 13th, 2022)</em> Proxy Tasks and Subjective Measures Can Be Misleading in Evaluating Explainable AI Systems. Zana Bucinca*, Phoebe Lin*, Krzysztof Gajos, Elena L. Glassman. IUI 2020. [Notes](notes/bucinca_iui_2020.md) -->
